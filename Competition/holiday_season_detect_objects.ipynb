{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "holiday-season.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP3FwrvNXY9lYqLKS3lguBO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chiranjeet14/ML_Projects/blob/master/Competition/holiday_season_detect_objects.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW26s6jy0luK"
      },
      "source": [
        "## Installing dependencies and notebook gpu setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFhhKNCg7rm5"
      },
      "source": [
        "# !pip uninstall -y tensorflow &> /dev/null\n",
        "# !pip install tensorflow &> /dev/null\n",
        "# !pip install --upgrade tensorflow &> /dev/null"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OObaJYLl0q2r"
      },
      "source": [
        "## Importing dependencies for the project"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "8aFCeQ3F0tP4",
        "outputId": "cbbb9709-14c6-48d1-80ec-cc66d241ce53"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "%matplotlib inline\n",
        "tf.__version__\n",
        "\n",
        "# from tensorflow.keras.preprocessing import image_dataset_from_directory"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESbmXn890xvB"
      },
      "source": [
        "## Data download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyRLrYRh0xFM",
        "outputId": "0f5b8f7d-22e5-481c-ac30-51a098de4494"
      },
      "source": [
        "import pathlib\n",
        "dataset_url = \"https://he-s3.s3.amazonaws.com/media/hackathon/hackerearth-deep-learning-challenge-holidays/holiday-season-11-2c924626/14feeca248c811eb.zip\"\n",
        "data_dir = tf.keras.utils.get_file('holiday-season', origin=dataset_url, extract=True)\n",
        "data_dir = pathlib.Path(data_dir)\n",
        "print(\"Data directory : \" + str(data_dir))\n",
        "# sub_directories = [x for x in data_dir.iterdir() if x.is_dir()]\n",
        "# print(sub_directories)\n",
        "# data_dir"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data directory : /root/.keras/datasets/holiday-season\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-_QzrnI8wOe"
      },
      "source": [
        "# data_dir = pathlib.Path(data_dir)\n",
        "# print(\"Data directory : \" + str(data_dir))\n",
        "# # sub_directories = [x for x in data_dir.iterdir() if x.is_dir()]\n",
        "# # print(sub_directories)\n",
        "# # data_dir"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0Fx17AM3ywZ",
        "outputId": "9f326c4d-3ffa-4e4a-9a74-dc9c4e0d419f"
      },
      "source": [
        "data_dir = pathlib.Path('/root/.keras/datasets/dataset')\n",
        "sub_directories = [x for x in data_dir.iterdir() if x.is_dir()]\n",
        "print(sub_directories)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[PosixPath('/root/.keras/datasets/dataset/train'), PosixPath('/root/.keras/datasets/dataset/test')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiwzSqXT36fo",
        "outputId": "c8e1a329-593d-4a65-9177-3f9414417991"
      },
      "source": [
        "train_folder = data_dir.joinpath('train')\n",
        "test_folder = data_dir.joinpath('test')\n",
        "print(\"Train dir : \" + str(train_folder))\n",
        "print(\"Test dir : \" + str(test_folder))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train dir : /root/.keras/datasets/dataset/train\n",
            "Test dir : /root/.keras/datasets/dataset/test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IoqvWDc1MGw",
        "outputId": "593215d5-e3ab-4c08-e790-2aedcfea6fbd"
      },
      "source": [
        "train_files = [e for e in train_folder.iterdir() if e.is_file()]\n",
        "test_files = [e for e in test_folder.iterdir() if e.is_file()]\n",
        "print(\"Train images count : \" + str(len(train_files)))\n",
        "print(\"Test images count : \" + str(len(test_files)))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train images count : 6469\n",
            "Test images count : 3489\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "LfKlaEkI37uQ",
        "outputId": "4ee58e28-6f75-4262-b573-f154b31828ff"
      },
      "source": [
        "train_csv_path = data_dir.joinpath('train.csv')\n",
        "#test_csv_path = data_dir.joinpath('test.csv')\n",
        "df_train = pd.read_csv(train_csv_path)\n",
        "# df_test = pd.read_csv(test_csv_path)\n",
        "df_train.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>image3476.jpg</td>\n",
              "      <td>Miscellaneous</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>image5198.jpg</td>\n",
              "      <td>Candle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>image4183.jpg</td>\n",
              "      <td>Snowman</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>image1806.jpg</td>\n",
              "      <td>Miscellaneous</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>image7831.jpg</td>\n",
              "      <td>Miscellaneous</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           Image          Class\n",
              "0  image3476.jpg  Miscellaneous\n",
              "1  image5198.jpg         Candle\n",
              "2  image4183.jpg        Snowman\n",
              "3  image1806.jpg  Miscellaneous\n",
              "4  image7831.jpg  Miscellaneous"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE2t3x5luBs6"
      },
      "source": [
        "## Creating training and validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laUjbA9Ouuqt"
      },
      "source": [
        "IMG_SIZE = (224, 224) # (img_height, img_width)\n",
        "IMG_SHAPE = IMG_SIZE + (3,)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRiGzLjJuC8J",
        "outputId": "021ae1b7-9940-465b-c09e-a240ba9ec2b0"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "preprocess_input = tf.keras.applications.inception_resnet_v2.preprocess_input\n",
        "\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(#rescale = 1./255,\n",
        "                                                                rotation_range=30,\n",
        "                                                                width_shift_range=0.2,\n",
        "                                                                height_shift_range=0.2,\n",
        "                                                                shear_range = 0.2,\n",
        "                                                                zoom_range = 0.2,\n",
        "                                                                horizontal_flip = True,\n",
        "                                                                vertical_flip = True,\n",
        "                                                                preprocessing_function=preprocess_input,\n",
        "                                                                validation_split=0.08,\n",
        "                                                                )\n",
        "\n",
        "# valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255,\n",
        "#                                                                preprocessing_function=preprocess_input,\n",
        "#                                                                )\n",
        "# validation and test images will be similar\n",
        "\n",
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(#rescale = 1./255,\n",
        "                                                               preprocessing_function=preprocess_input,\n",
        "                                                               )\n",
        "\n",
        "# train_generator = train_datagen.flow_from_directory('data/train',\n",
        "#                                                     target_size=IMG_SIZE,\n",
        "#                                                     # color_mode='rgb',\n",
        "#                                                     batch_size=BATCH_SIZE,\n",
        "#                                                     class_mode='categorical',\n",
        "#                                                     shuffle=True,\n",
        "#                                                     )\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(dataframe=df_train,\n",
        "                                                    directory=train_folder,\n",
        "                                                    x_col=\"Image\",\n",
        "                                                    y_col=\"Class\",\n",
        "                                                    batch_size=BATCH_SIZE,\n",
        "                                                    shuffle=True,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    target_size=IMG_SIZE,\n",
        "                                                    subset=\"training\",\n",
        "                                                    )\n",
        "\n",
        "validation_generator = train_datagen.flow_from_dataframe(dataframe=df_train,\n",
        "                                                        directory=train_folder,\n",
        "                                                        x_col=\"Image\",\n",
        "                                                        y_col=\"Class\",\n",
        "                                                        batch_size=BATCH_SIZE,\n",
        "                                                        shuffle=True,\n",
        "                                                        class_mode='categorical',\n",
        "                                                        target_size=IMG_SIZE,\n",
        "                                                        subset=\"validation\",\n",
        "                                                        )\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory('/root/.keras/datasets/dataset',\n",
        "                                                  batch_size=10,\n",
        "                                                  # only read images from `test` directory\n",
        "                                                  classes=['test'],\n",
        "                                                  # don't generate labels\n",
        "                                                  class_mode=None,\n",
        "                                                  # don't shuffle\n",
        "                                                  shuffle=False,\n",
        "                                                  # use same size as in training\n",
        "                                                  target_size=IMG_SIZE,\n",
        "                                                  )"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 5952 validated image filenames belonging to 6 classes.\n",
            "Found 517 validated image filenames belonging to 6 classes.\n",
            "Found 3489 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5an45jia5NPO"
      },
      "source": [
        "## Creating the base model and add some extra layers to adjust to our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2NTTxeZ5Y7l"
      },
      "source": [
        "base_model = tf.keras.applications.InceptionResNetV2(input_shape=IMG_SHAPE,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet',\n",
        "                                               )"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR8Uwmhn1JRy"
      },
      "source": [
        "### Base model layers to train ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kx_M8Y20ALCr"
      },
      "source": [
        "base_model.trainable = True\n",
        "fine_tune_layers = 100"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLvVFzKS0pbL"
      },
      "source": [
        "# if base_model.trainable == True:\n",
        "#   total_base_model_layers = len(base_model.layers)\n",
        "#   print(\"Number of layers in the base model: \", str(total_base_model_layers))\n",
        "\n",
        "#   for layer in base_model.layers[:total_base_model_layers-fine_tune_layers]:\n",
        "#     layer.trainable = False"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JYuBmp4t9-e"
      },
      "source": [
        "x = base_model.output\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = tf.keras.layers.Dense(1024,activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.03),bias_regularizer=tf.keras.regularizers.L2(0.03))(x)\n",
        "x = tf.keras.layers.Dense(1024,activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.03),bias_regularizer=tf.keras.regularizers.L2(0.03))(x)\n",
        "x = tf.keras.layers.Dense(1024,activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.03),bias_regularizer=tf.keras.regularizers.L2(0.03))(x)\n",
        "x = tf.keras.layers.Dropout(0.5)(x)\n",
        "x = tf.keras.layers.Dense(512,activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.03),bias_regularizer=tf.keras.regularizers.L2(0.03))(x)\n",
        "x = tf.keras.layers.Dense(512,activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.03),bias_regularizer=tf.keras.regularizers.L2(0.03))(x)\n",
        "x = tf.keras.layers.Dense(512,activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.03),bias_regularizer=tf.keras.regularizers.L2(0.03))(x)\n",
        "x = tf.keras.layers.Dropout(0.3)(x)\n",
        "preds = tf.keras.layers.Dense(6,activation='softmax')(x)\n",
        "model = tf.keras.models.Model(inputs=base_model.input,outputs=preds)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk5ZtD7P7lqy"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEdgQzcf7Hp3"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9),loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001, epsilon=0.1), loss='categorical_crossentropy', metrics = ['accuracy'])\n",
        "# model.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utGOcbnU7sW7",
        "outputId": "06e09cf8-82b9-4e93-d5fc-be919a78a105"
      },
      "source": [
        "STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size\n",
        "STEP_SIZE_VALID = validation_generator.n//validation_generator.batch_size\n",
        "EPOCHS = 100\n",
        "\n",
        "# Save the model according to the conditions\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\"test_model.h5\", monitor='val_accuracy', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', save_freq='epoch')\n",
        "early = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, patience=10, verbose=0, mode='auto')\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(train_generator,\n",
        "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                    validation_data=validation_generator,\n",
        "                    validation_steps=STEP_SIZE_VALID,\n",
        "                    epochs=EPOCHS,\n",
        "                    callbacks = [checkpoint, early],\n",
        "                    )\n",
        "model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "160/186 [========================>.....] - ETA: 14s - loss: 145.5027 - accuracy: 0.3842"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ulpq6fYaDBwr"
      },
      "source": [
        "## Predicting on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-V3DhOhQDCbl"
      },
      "source": [
        "predictions = model.predict_generator(test_generator,verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkF3WRc3a2S6"
      },
      "source": [
        "## Predictions to output.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHHuc9OYXuue"
      },
      "source": [
        "test_filenames = []\n",
        "for filename in test_generator.filenames: \n",
        "    filename = filename.replace('test/','')\n",
        "    test_filenames.append(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7bBDqxPU_1A"
      },
      "source": [
        "preds_cls_idx = predictions.argmax(axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJhR2WdLVJDT"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "idx_to_classes = {v: k for k, v in train_generator.class_indices.items()}\n",
        "preds_classes = np.vectorize(idx_to_classes.get)(preds_cls_idx)\n",
        "filenames_to_classes = list(zip(test_filenames, preds_classes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5ilnoqFVPCv"
      },
      "source": [
        "# filenames_to_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TucqgTCDZ0CK"
      },
      "source": [
        "data = pd.DataFrame((zip(test_filenames, preds_classes)),columns=['Image','Class'])\n",
        "data.to_csv('output.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7vFeDFLaIKx"
      },
      "source": [
        "df_demo = pd.read_csv('output.csv')\n",
        "df_demo.head()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}